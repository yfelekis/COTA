{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Abstraction - learning a commuting diagram with a NN\n",
    "\n",
    "In this notebook we switch from studying abstractions from the point of view of their definition, their properties and their implications, to considering learning problems. Our attention will focus on how abstractions (as they were explained in the previous notebooks) may be learned from data. We will start define artificial problems and we will examine how they can be solved using machine learning models.\n",
    "\n",
    "In this notebook we review how we could learn the entire commuting diagrams starting from data collected from two models (a low-level model and a high-level model) undergoing corresponding interventions. We will solve the learning problem deploying simple neural networks with internal paths tailored to reflect the shape of our commuting diagrams.\n",
    "\n",
    "This notebook was developed in order to exploit the framework introduced in [Rischel2020] in order to perform learning. The notebook is structured as follows: \n",
    "- Import of relevant modules and classes (Section 2)\n",
    "- Definition of the problem (Section 3)\n",
    "- First case study (Section 4)\n",
    "- Second case study (Section 5)\n",
    "\n",
    "DISCLAIMER 1: the notebook refers to ideas from *causality* and *category theory* for which only a quick definition is offered. Useful references for causality are [Pearl2009,Peters2017], while for category theory are [Spivak2014,Fong2018].\n",
    "\n",
    "DISCLAIMER 2: mistakes are in all likelihood due to misunderstandings by the notebook author in reading [Rischel2020] and/or [Rubenstein2017]. Feedback very welcome! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard libraries\n",
    "We start importing standard and custom libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from pgmpy.models import BayesianNetwork as BN\n",
    "from pgmpy.factors.discrete import TabularCPD as cpd\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.SCMMappings import Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, and for discussing our results in this notebook, we set a random seed to $1985$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1985)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a number of samples to be collected in simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10**4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "## Assumptions\n",
    "We assume that two research groups (LOW and HIGH) have developed different models of a same system in the following scenario:\n",
    "1. The two models represent the same system with different levels of details;\n",
    "2. The two groups do not know the analytic form of their model (they can draw the DAG of causal dependencies, but the do not know the form of the structural equations);\n",
    "3. The two groups can still sample data from the model (they can run a black-box model or collect data form the real-world systems they are modelling);\n",
    "4. The two groups are interested in a particular interventional distribution (typically they both want to know the distribution of an outcome given an intervention on a treatment);\n",
    "5. The two groups are aware on how to align their interventions (they know how manipulations of the treatment variables relate in the two models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Let us consider our usual commuting diagrams. Given (interventional) data $\\mathcal{D}$, we want to learn the functions making it commute:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathcal{\\mathcal{M}_{do}}\\left[\\mathbf{A}\\right] & \\overset{\\mathcal{\\mathcal{M}_{do}}\\left[\\phi_{\\mathbf{B}}\\right]}{\\longrightarrow} & \\mathcal{\\mathcal{M}_{do}}\\left[\\mathbf{B}\\right]\\\\\n",
    "\\sideset{}{\\alpha_{\\mathbf{X}}}\\downarrow &  & \\sideset{}{\\alpha_{\\mathbf{Y}}}\\downarrow\\\\\n",
    "\\mathcal{\\mathcal{M'}_{do}}\\left[\\mathbf{X}\\right] & \\overset{\\mathcal{\\mathcal{M'}_{do}}\\left[\\phi_{\\mathbf{Y}}\\right]}{\\longrightarrow} & \\mathcal{\\mathcal{M'}_{do}}\\left[\\mathbf{Y}\\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{M}_{do}\\left[\\mathbf{X}\\right]$ and $\\mathcal{M}_{do}\\left[\\mathbf{Y}\\right]$: are disjoint sub-sets of variables in the high-level model;\n",
    "- $\\mathcal{M}_{do}\\left[\\mathbf{A}\\right]$ and $\\mathcal{M}_{do}\\left[\\mathbf{A}\\right]$: are the corresponding sub-sets of variables in the high-level model;\n",
    "- $\\mathcal{M}_{do}\\left[\\phi_\\mathbf{Y}\\right]$ and $\\mathcal{M}_{do}\\left[\\phi_\\mathbf{B}\\right]$: are the mechanisms;\n",
    "- $\\alpha_{\\mathbf{X}}$ and $\\alpha_{\\mathbf{Y}}$: are the abstractions.\n",
    "\n",
    "In other words the two research groups (LOW and HIGH) want to learn at the same time (i) the mechanisms in their models and (ii) the relation of abstraction between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The two group are going to sample aligned data. Following Assumption (4) this means that when group LOW perform the intervention of interest ($do(\\mathbf{a})$) and collects data ($\\mathbf{a},\\mathbf{b}$), group HIGH performs the corresponding interventions ($do(\\mathbf{x})$) and collects its own data ($\\mathbf{x},\\mathbf{y}$).\n",
    "\n",
    "These two pieces of data are then assembled in a single datasample in $\\mathcal{D}$ that takes the form of a tuple:\n",
    "$$ (\\mathbf{a}, \\mathbf{b}, \\mathbf{x}, \\mathbf{y}) $$\n",
    "where:\n",
    "- $\\mathbf{b} = \\mathcal{M}_{do}\\left[\\phi_{\\mathbf{B}}\\right](\\mathbf{a})$\n",
    "- $\\mathbf{x} = \\alpha_{\\mathbf{X}}(\\mathbf{a})$\n",
    "- $\\mathbf{y} = \\mathcal{M'}_{do}\\left[\\phi_{\\mathbf{Y}}\\right](\\mathbf{x}) = \\alpha_{\\mathbf{Y}}(\\mathbf{b})$\n",
    "\n",
    "Notice that the fact that the two groups know how to align interventions, does not necessarily means they know the function $\\alpha_X$; their knowledge of this function may be restricted to a small subset of interventions; by learning the whole commuting diagram knowledge of $\\alpha_X$ may be generalized to the whole domain of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "We implement a neural network which takes in input $\\mathbf{a}$ and $\\mathbf{x}$ learns $\\mathcal{M}_{do}\\left[\\phi_{\\mathbf{B}}\\right]$, $\\mathcal{M'}_{do}\\left[\\phi_{\\mathbf{Y}}\\right]$, $\\alpha_{\\mathbf{X}}$ and $\\alpha_{\\mathbf{Y}}$ by trying to predict $\\mathbf{\\hat{b}}, \\mathbf{\\hat{y}}, \\mathbf{\\hat{a}}$ and trying to match the result of the upper path and the lower path of the commuting diagram.\n",
    "\n",
    "Specifically we define the loss function as:\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{l}(\\mathbf{\\hat{b}},\\mathbf{{b}}) + \\mathcal{l}(\\mathbf{\\hat{y}},\\mathbf{{y}}) + \\mathcal{l}(\\mathbf{\\hat{a}},\\mathbf{{a}}) + \\mathcal{l}(\\mathbf{\\hat{y}}_{up},\\mathbf{\\hat{y}}_{low})\n",
    "$$\n",
    "where:\n",
    "- $\\mathcal{l}(\\mathbf{\\hat{b}},\\mathbf{{b}})$ is the prediction error for the low-level mechanism\n",
    "- $\\mathcal{l}(\\mathbf{\\hat{y}},\\mathbf{{y}})$ is the prediction error for the high-level mechanism\n",
    "- $\\mathcal{l}(\\mathbf{\\hat{a}},\\mathbf{{a}})$ is the prediction error for the abstraction on $A$\n",
    "- $\\mathcal{l}(\\mathbf{\\hat{y}}_{up},\\mathbf{\\hat{y}}_{low})$ is the prediction error for the commutativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of models and abstraction \n",
    "\n",
    "We define the standard *lung cancer scenario* models we have used in the previous notebooks. For a detailed description, see the first notebook *Categorical Abstraction.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M0 = BN([('Smoking','Tar'),('Tar','Cancer')])\n",
    "\n",
    "cpdS = cpd(variable='Smoking',\n",
    "          variable_card=2,\n",
    "          values=[[.8],[.2]],\n",
    "          evidence=None,\n",
    "          evidence_card=None)\n",
    "cpdT = cpd(variable='Tar',\n",
    "          variable_card=2,\n",
    "          values=[[1,.2],[0.,.8]],\n",
    "          evidence=['Smoking'],\n",
    "          evidence_card=[2])\n",
    "cpdC = cpd(variable='Cancer',\n",
    "          variable_card=2,\n",
    "          values=[[.9,.6],[.1,.4]],\n",
    "          evidence=['Tar'],\n",
    "          evidence_card=[2])\n",
    "\n",
    "M0.add_cpds(cpdS,cpdT,cpdC)\n",
    "M0.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = BN([('Smoking_','Cancer_')])\n",
    "\n",
    "cpdS = cpd(variable='Smoking_',\n",
    "          variable_card=2,\n",
    "          values=[[.8],[.2]],\n",
    "          evidence=None,\n",
    "          evidence_card=None)\n",
    "cpdC = cpd(variable='Cancer_',\n",
    "          variable_card=2,\n",
    "          values=[[.9,.66],[.1,.34]],\n",
    "          evidence=['Smoking_'],\n",
    "          evidence_card=[2])\n",
    "\n",
    "M1.add_cpds(cpdS,cpdC)\n",
    "M1.check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the abstraction between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = ['Smoking','Cancer']\n",
    "\n",
    "a = {'Smoking': 'Smoking_',\n",
    "    'Cancer': 'Cancer_'}\n",
    "alphas = {'Smoking_': np.eye(2),\n",
    "         'Cancer_': np.eye(2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Abstraction(M0,M1,R,a,alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then pay attention to the interventional distribution of interest $P(C'\\vert do(S'))$ and learn the following diagram:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathcal{\\mathcal{M}_{do}}\\left[{S}\\right] & \\overset{\\mathcal{\\mathcal{M}_{do}}\\left[\\phi_{\\tilde{C}}\\right]}{\\longrightarrow} & \\mathcal{\\mathcal{M}_{do}}\\left[C\\right]\\\\\n",
    "\\sideset{}{\\alpha_{S'}}\\downarrow &  & \\sideset{}{\\alpha_{C'}}\\downarrow\\\\\n",
    "\\mathcal{\\mathcal{M'}_{do}}\\left[S'\\right] & \\overset{\\mathcal{\\mathcal{M'}_{do}}\\left[\\phi_{C'}\\right]}{\\longrightarrow} & \\mathcal{\\mathcal{M'}_{do}}\\left[C'\\right]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "We generate data from the low-level model for learning. This will provide the first part $(\\mathbf{a},\\mathbf{b})$ of our datapoints. For readability we change the notation from $(\\mathbf{a},\\mathbf{b})$ to $(\\mathbf{s},\\mathbf{c})$ to denote samples of smoking and cancer from the low-level model $\\mathcal{M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Tar  Smoking  Cancer\n",
      "0       0        0       0\n",
      "1       0        0       0\n",
      "2       0        0       0\n",
      "3       0        0       0\n",
      "4       0        0       0\n",
      "...   ...      ...     ...\n",
      "9995    0        0       0\n",
      "9996    0        0       0\n",
      "9997    0        0       0\n",
      "9998    0        0       0\n",
      "9999    0        0       0\n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "M0do = M0.do('Smoking')\n",
    "lowlevel_data = M0do.simulate(n_samples=n_samples, show_progress=False)\n",
    "print(lowlevel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we generate data from the high-level model. This is meant to provide the second part $(\\mathbf{x},\\mathbf{y})$ or $(\\mathbf{s'},\\mathbf{c'})$ of our datapoints. Notice that we need to align correctly our datapoints since we want $\\mathbf{s'} = \\alpha_{S}(\\mathbf{s})$; therefore we generate the datapoints one by one.\n",
    "\n",
    "**TODO: this generation method is highly inefficient!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Cancer_  Smoking_\n",
      "0         0         0\n",
      "0         0         0\n",
      "0         0         0\n",
      "0         0         0\n",
      "0         0         0\n",
      "..      ...       ...\n",
      "0         0         0\n",
      "0         0         0\n",
      "0         0         0\n",
      "0         0         0\n",
      "0         0         0\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "M1do = M1.do('Smoking_')\n",
    "\n",
    "high_level_samples = [M1do.simulate(n_samples=1, evidence={'Smoking_': lowlevel_data.loc[i]['Smoking']}, show_progress=False) for i in range(lowlevel_data.shape[0])]\n",
    "highlevel_data = pd.concat(high_level_samples)\n",
    "    \n",
    "print(highlevel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning model definition\n",
    "\n",
    "We now define a simple neural network with the modules and loss functions described above.\n",
    "\n",
    "We start with some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "hiddensize = 4\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify its architecture. Notice how the layes are structured to mimic the arrows in our commuting diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1,hiddensize)\n",
    "        self.sm1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hiddensize,1)\n",
    "        self.lowmech = nn.Sigmoid()\n",
    "        \n",
    "        self.fc3 = nn.Linear(1,hiddensize)\n",
    "        self.sm2 = nn.Sigmoid()\n",
    "        self.fc4 = nn.Linear(hiddensize,1)\n",
    "        self.absS = nn.Sigmoid()\n",
    "        \n",
    "        self.fc5 = nn.Linear(1,hiddensize)\n",
    "        self.sm3 = nn.Sigmoid()\n",
    "        self.fc6 = nn.Linear(hiddensize,1)\n",
    "        self.highmech = nn.Sigmoid()\n",
    "        \n",
    "        self.fc7 = nn.Linear(1,hiddensize)\n",
    "        self.sm4 = nn.Sigmoid()\n",
    "        self.fc8 = nn.Linear(hiddensize,1)\n",
    "        self.absC = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_lowS, input_highS):\n",
    "        out = self.fc1(input_lowS)\n",
    "        out = self.sm1(out)\n",
    "        out = self.fc2(out)\n",
    "        mech_lowS = self.lowmech(out)\n",
    "        \n",
    "        out = self.fc3(input_lowS)\n",
    "        out = self.sm2(out)\n",
    "        out = self.fc4(out)\n",
    "        abs_lowS = self.absS(out)\n",
    "        \n",
    "        out = self.fc5(input_highS)\n",
    "        out = self.sm3(out)\n",
    "        out = self.fc6(out)\n",
    "        mech_highS = self.highmech(out)\n",
    "        \n",
    "        out = self.fc5(abs_lowS)\n",
    "        out = self.sm3(out)\n",
    "        out = self.fc6(out)\n",
    "        mech_abs_lowS = self.highmech(out)\n",
    "        \n",
    "        out = self.fc7(mech_lowS)\n",
    "        out = self.sm4(out)\n",
    "        out = self.fc8(out)\n",
    "        abs_mech_lowS = self.absS(out)\n",
    "        \n",
    "        return mech_lowS, abs_lowS, mech_highS, abs_mech_lowS, mech_abs_lowS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model and its optimizer. We use the simplest loss function provided by *pytorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss_lowmech = nn.MSELoss()\n",
    "loss_highmech = nn.MSELoss()\n",
    "loss_abstraction = nn.MSELoss()\n",
    "loss_commutativity = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train the network with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/2000], Loss: 0.6090\n",
      "Epoch [100/2000], Loss: 0.4534\n",
      "Epoch [150/2000], Loss: 0.4034\n",
      "Epoch [200/2000], Loss: 0.3788\n",
      "Epoch [250/2000], Loss: 0.3569\n",
      "Epoch [300/2000], Loss: 0.3336\n",
      "Epoch [350/2000], Loss: 0.3112\n",
      "Epoch [400/2000], Loss: 0.2922\n",
      "Epoch [450/2000], Loss: 0.2778\n",
      "Epoch [500/2000], Loss: 0.2676\n",
      "Epoch [550/2000], Loss: 0.2604\n",
      "Epoch [600/2000], Loss: 0.2553\n",
      "Epoch [650/2000], Loss: 0.2517\n",
      "Epoch [700/2000], Loss: 0.2490\n",
      "Epoch [750/2000], Loss: 0.2470\n",
      "Epoch [800/2000], Loss: 0.2454\n",
      "Epoch [850/2000], Loss: 0.2442\n",
      "Epoch [900/2000], Loss: 0.2432\n",
      "Epoch [950/2000], Loss: 0.2424\n",
      "Epoch [1000/2000], Loss: 0.2417\n",
      "Epoch [1050/2000], Loss: 0.2411\n",
      "Epoch [1100/2000], Loss: 0.2406\n",
      "Epoch [1150/2000], Loss: 0.2401\n",
      "Epoch [1200/2000], Loss: 0.2397\n",
      "Epoch [1250/2000], Loss: 0.2393\n",
      "Epoch [1300/2000], Loss: 0.2390\n",
      "Epoch [1350/2000], Loss: 0.2386\n",
      "Epoch [1400/2000], Loss: 0.2383\n",
      "Epoch [1450/2000], Loss: 0.2379\n",
      "Epoch [1500/2000], Loss: 0.2376\n",
      "Epoch [1550/2000], Loss: 0.2373\n",
      "Epoch [1600/2000], Loss: 0.2370\n",
      "Epoch [1650/2000], Loss: 0.2367\n",
      "Epoch [1700/2000], Loss: 0.2364\n",
      "Epoch [1750/2000], Loss: 0.2361\n",
      "Epoch [1800/2000], Loss: 0.2358\n",
      "Epoch [1850/2000], Loss: 0.2356\n",
      "Epoch [1900/2000], Loss: 0.2353\n",
      "Epoch [1950/2000], Loss: 0.2351\n",
      "Epoch [2000/2000], Loss: 0.2349\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    input_lowS = torch.from_numpy(np.expand_dims(lowlevel_data['Smoking'].to_numpy(dtype=np.float32),axis=1))\n",
    "    input_highS = torch.from_numpy(np.expand_dims(highlevel_data['Smoking_'].to_numpy(dtype=np.float32),axis=1))\n",
    "    target_lowC = torch.from_numpy(np.expand_dims(lowlevel_data['Cancer'].to_numpy(dtype=np.float32),axis=1))\n",
    "    target_highC = torch.from_numpy(np.expand_dims(highlevel_data['Cancer_'].to_numpy(dtype=np.float32),axis=1))\n",
    "    \n",
    "    mech_lowS, abs_lowS, mech_highS, abs_mech_lowS, mech_abs_lowS  = model(input_lowS, input_highS)\n",
    "    \n",
    "    l1 = loss_lowmech(mech_lowS,target_lowC)\n",
    "    l2 = loss_highmech(mech_highS,target_highC)\n",
    "    l3 = loss_abstraction(abs_lowS,input_highS)\n",
    "    l4 = loss_commutativity(abs_mech_lowS,mech_abs_lowS)                                \n",
    "    loss = l1+l2+l3+l4\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We define a test function to output a readable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smalltest(model, test_lowS,test_highS):\n",
    "    mech_lowS, abs_lowS, mech_highS, abs_mech_lowS, mech_abs_lowS = model(test_lowS,test_highS)\n",
    "    \n",
    "    print('-------------------------')\n",
    "    print('Given I perform the intervention S={0}'.format(test_lowS.item()))\n",
    "    print('The prediction of the low level mechanism P_M0 (C=1 | S={0}) outputs {1}'.format(test_lowS.item(),mech_lowS.item()))\n",
    "    \n",
    "    print('\\nThe prediction of the abstraction to high-level alpha_S({0}) outputs {1} against the expected {2}'.format(test_lowS.item(),abs_lowS.item(),test_highS.item()))\n",
    "    \n",
    "    print('\\nThe prediction of the high level mechanism P_M1 (C=1 | S={0}) outputs {1} using the input {2}'.format(test_highS.item(),mech_highS.item(),test_highS.item()))\n",
    "    print('The prediction of the high level mechanism P_M1 (C=1 | S={0}) outputs {1} using the transformed input {2}'.format(abs_lowS.item(),mech_abs_lowS.item(),test_lowS.item()))\n",
    "    \n",
    "    print('\\nThe upper path produces {0}'.format(mech_abs_lowS.item()))\n",
    "    print('The lower path produces {0}'.format(abs_mech_lowS.item()))\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check what our network produces with an input of $(\\mathbf{s}=1,\\mathbf{s'}=1)$. Notice that $\\mathcal{s'}$ is optional, but we will use it to see how the correct high-level interventions would be processed by the network.\n",
    "\n",
    "Remember we are trying to learn $\\mathcal{M}[\\phi_\\tilde{C}]$, $\\mathcal{M'}[\\phi_{C'}]$ which is the same as $\\mathcal{M}[\\phi_\\tilde{C}]$, and $\\alpha_{C}$ and $\\alpha_{C'}$ which are both identities. As a reference, we first output the value of the mechanism $\\mathcal{M}[\\phi_\\tilde{C}] = \\mathcal{M}[\\phi_{C'}] = P(Cancer|do(Smoking))$ which we are trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9 , 0.1 ],\n",
       "       [0.66, 0.34]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference = VariableElimination(M0do)\n",
    "M0_joint_TS = inference.query(['Cancer','Smoking'],show_progress=False)\n",
    "M0_joint_S = inference.query(['Smoking'],show_progress=False)\n",
    "M0_cond_TS = M0_joint_TS / M0_joint_S\n",
    "M0_cond_TS.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above matrix, we know that under intervention $S=1$, $P(C=1 \\vert do(S=1)) = 0.34$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Given I perform the intervention S=1.0\n",
      "The prediction of the low level mechanism P_M0 (C=1 | S=1.0) outputs 0.37025806307792664\n",
      "\n",
      "The prediction of the abstraction to high-level alpha_S(1.0) outputs 0.943284273147583 against the expected 1.0\n",
      "\n",
      "The prediction of the high level mechanism P_M1 (C=1 | S=1.0) outputs 0.3083462417125702 using the input 1.0\n",
      "The prediction of the high level mechanism P_M1 (C=1 | S=0.943284273147583) outputs 0.29659566283226013 using the transformed input 1.0\n",
      "\n",
      "The upper path produces 0.29659566283226013\n",
      "The lower path produces 0.2572880983352661\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "test_lowS = torch.from_numpy(np.array([1.],dtype=np.float32))\n",
    "test_highS = torch.from_numpy(np.array([1.],dtype=np.float32))\n",
    "\n",
    "smalltest(model, test_lowS,test_highS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network roughly learnt our diagram:\n",
    "- The prediction $\\hat{P}_\\mathcal{M}(C=1 \\vert do(S=1))$ is close to $0.34$;\n",
    "- The prediction $\\alpha_S(1)$ is close to $1$;\n",
    "- The prediction $\\hat{P}_\\mathcal{M'}(C'=1 \\vert do(S'=1))$ is roughly around $0.34$ when using the input data $\\mathbf{s'}$; it is further away when using $\\alpha_S(\\mathbf{s})$ because of accumulation of approximations.\n",
    "- The results along the upper and lower path are close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could perform the same analysis for the intervention $S=0$, where we expect $P(C=1 \\vert do(S=1)) = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Given I perform the intervention S=0.0\n",
      "The prediction of the low level mechanism P_M0 (C=1 | S=0.0) outputs 0.09720170497894287\n",
      "\n",
      "The prediction of the abstraction to high-level alpha_S(0.0) outputs 0.021648824214935303 against the expected 0.0\n",
      "\n",
      "The prediction of the high level mechanism P_M1 (C=1 | S=0.0) outputs 0.10958739370107651 using the input 0.0\n",
      "The prediction of the high level mechanism P_M1 (C=1 | S=0.021648824214935303) outputs 0.1124633401632309 using the transformed input 0.0\n",
      "\n",
      "The upper path produces 0.1124633401632309\n",
      "The lower path produces 0.1256360113620758\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "test_lowS = torch.from_numpy(np.array([0.],dtype=np.float32))\n",
    "test_highS = torch.from_numpy(np.array([0.],dtype=np.float32))\n",
    "\n",
    "smalltest(model, test_lowS,test_highS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again confirms learning:\n",
    "- The prediction $\\hat{P}_\\mathcal{M}(C=1 \\vert do(S=0))$ is roughly around $0.1$;\n",
    "- The prediction $\\alpha_S(0)$ is close to $0$;\n",
    "- The prediction $\\hat{P}_\\mathcal{M'}(C'=1 \\vert do(S'=0))$ is roughly around $0.1$ when using the input data $\\mathbf{s'}$; it is further away when using $\\alpha_S(\\mathbf{s})$ because of accumulation of approximations.\n",
    "- The results along the upper and lower path are close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of models and abstraction \n",
    "\n",
    "We define an alternative synthetic case with a slightly more complex underlying graph. In particular, we will consider a base model where variables will be collapsed ($B,C \\mapsto Y$), where domains in the base and abstract models are different ($\\mathcal{M}[A] \\neq \\mathcal{M'}[X]$) forcing the abstraction to be different from an identity, and where the abstraction has error different from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M0 = BN([('A','B'),('A','C'),('B','D'),('C','D')])\n",
    "\n",
    "cpdA = cpd(variable='A',\n",
    "          variable_card=3,\n",
    "          values=[[.7],[.15],[.15]],\n",
    "          evidence=None,\n",
    "          evidence_card=None)\n",
    "cpdB = cpd(variable='B',\n",
    "          variable_card=2,\n",
    "          values=[[.7,.2,.3],[.3,.8,.7]],\n",
    "          evidence=['A'],\n",
    "          evidence_card=[3])\n",
    "cpdC = cpd(variable='C',\n",
    "          variable_card=2,\n",
    "          values=[[.9,.2,.25],[.1,.8,.75]],\n",
    "          evidence=['A'],\n",
    "          evidence_card=[3])\n",
    "cpdD = cpd(variable='D',\n",
    "          variable_card=2,\n",
    "          values=[[.8,.2,.15,.8],[.2,.8,.85,.2]],\n",
    "          evidence=['B','C'],\n",
    "          evidence_card=[2,2])\n",
    "\n",
    "M0.add_cpds(cpdA,cpdB,cpdC,cpdD)\n",
    "M0.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = BN([('X','Y'),('Y','Z')])\n",
    "\n",
    "cpdX = cpd(variable='X',\n",
    "          variable_card=2,\n",
    "          values=[[.7],[.3]],\n",
    "          evidence=None,\n",
    "          evidence_card=None)\n",
    "cpdY = cpd(variable='Y',\n",
    "          variable_card=2,\n",
    "          values=[[1,0],[0,1]],\n",
    "          evidence=['X'],\n",
    "          evidence_card=[2])\n",
    "cpdZ = cpd(variable='Z',\n",
    "          variable_card=2,\n",
    "          values=[[0.4025,0.39],[0.5975,0.61]],\n",
    "          evidence=['Y'],\n",
    "          evidence_card=[2])\n",
    "\n",
    "M1.add_cpds(cpdX,cpdY,cpdZ)\n",
    "M1.check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the abstraction between the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = ['A','B','C','D']\n",
    "\n",
    "a = {'A': 'X',\n",
    "    'B': 'Y',\n",
    "    'C': 'Y',\n",
    "    'D': 'Z'}\n",
    "alphas = {'X': np.array([[1,0,0],[0,1,1]]),\n",
    "         'Y': np.array([[1,0,0,1],[0,1,1,0]]),\n",
    "         'Z': np.eye(2),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Abstraction(M0,M1,R,a,alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then pay attention to the interventional distribution of interest $P(Z\\vert do(X))$ and learn the following diagram:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathcal{\\mathcal{M}_{do}}\\left[{A}\\right] & \\overset{\\mathcal{\\mathcal{M}_{do}}\\left[\\phi_{\\tilde{D}}\\right]}{\\longrightarrow} & \\mathcal{\\mathcal{M}_{do}}\\left[D\\right]\\\\\n",
    "\\sideset{}{\\alpha_{X}}\\downarrow &  & \\sideset{}{\\alpha_{Z}}\\downarrow\\\\\n",
    "\\mathcal{\\mathcal{M}_{do}}\\left[X\\right] & \\overset{\\mathcal{\\mathcal{M'}_{do}}\\left[\\phi_{Z}\\right]}{\\longrightarrow} & \\mathcal{\\mathcal{M'}_{do}}\\left[Z\\right]\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference we also computer the abstraction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Checking ['X'] -> ['Y']: True\n",
      "- Checking ['X'] -> ['Z']: True\n",
      "- Checking ['X'] -> ['Y', 'Z']: True\n",
      "- Checking ['Y'] -> ['X']: False\n",
      "---- Checking ['B', 'C'] -> ['A']: False\n",
      "- Checking ['Y'] -> ['Z']: True\n",
      "- Checking ['Y'] -> ['X', 'Z']: True\n",
      "- Checking ['Z'] -> ['X']: False\n",
      "---- Checking ['D'] -> ['A']: False\n",
      "- Checking ['Z'] -> ['Y']: False\n",
      "---- Checking ['D'] -> ['B', 'C']: False\n",
      "- Checking ['Z'] -> ['X', 'Y']: False\n",
      "---- Checking ['D'] -> ['A', 'B', 'C']: False\n",
      "- Checking ['X', 'Y'] -> ['Z']: True\n",
      "- Checking ['X', 'Z'] -> ['Y']: True\n",
      "- Checking ['Y', 'Z'] -> ['X']: False\n",
      "---- Checking ['B', 'C', 'D'] -> ['A']: False\n",
      "\n",
      " 7 legitimate pairs of sets out of 49 possbile pairs of sets\n",
      "\n",
      "M1: ['X'] -> ['Y']\n",
      "M0: ['A'] -> ['B', 'C']\n",
      "M1 mechanism shape: (2, 2)\n",
      "M0 mechanism shape: (4, 3)\n",
      "Alpha_s shape: (2, 3)\n",
      "Alpha_t shape: (2, 4)\n",
      "All JS distances: [0.36792454944836245, 0.5723641752371845, 0.523792390695269]\n",
      "\n",
      "Abstraction error: 0.5723641752371845\n",
      "\n",
      "M1: ['X'] -> ['Z']\n",
      "M0: ['A'] -> ['D']\n",
      "M1 mechanism shape: (2, 2)\n",
      "M0 mechanism shape: (2, 3)\n",
      "Alpha_s shape: (2, 3)\n",
      "Alpha_t shape: (2, 2)\n",
      "All JS distances: [0.12764169290832625, 0.14905469732969073, 0.11447249720060491]\n",
      "\n",
      "Abstraction error: 0.14905469732969073\n",
      "\n",
      "M1: ['X'] -> ['Y', 'Z']\n",
      "M0: ['A'] -> ['B', 'C', 'D']\n",
      "M1 mechanism shape: (4, 2)\n",
      "M0 mechanism shape: (8, 3)\n",
      "Alpha_s shape: (2, 3)\n",
      "Alpha_t shape: (4, 8)\n",
      "All JS distances: [0.44993580821834983, 0.5841050035045615, 0.5384568391200848]\n",
      "\n",
      "Abstraction error: 0.5841050035045615\n",
      "\n",
      "M1: ['Y'] -> ['Z']\n",
      "M0: ['B', 'C'] -> ['D']\n",
      "M1 mechanism shape: (2, 2)\n",
      "M0 mechanism shape: (2, 4)\n",
      "Alpha_s shape: (2, 4)\n",
      "Alpha_t shape: (2, 2)\n",
      "All JS distances: [0.2920517223899882, 0.14829984047274675, 0.19372726395553327, 0.2920517223899879]\n",
      "\n",
      "Abstraction error: 0.2920517223899882\n",
      "\n",
      "M1: ['Y'] -> ['X', 'Z']\n",
      "M0: ['B', 'C'] -> ['A', 'D']\n",
      "M1 mechanism shape: (4, 2)\n",
      "M0 mechanism shape: (6, 4)\n",
      "Alpha_s shape: (2, 4)\n",
      "Alpha_t shape: (4, 6)\n",
      "All JS distances: [0.292051722389988, 0.14829984047274716, 0.19372726395553322, 0.2920517223899881]\n",
      "\n",
      "Abstraction error: 0.2920517223899881\n",
      "\n",
      "M1: ['X', 'Y'] -> ['Z']\n",
      "M0: ['A', 'B', 'C'] -> ['D']\n",
      "M1 mechanism shape: (2, 4)\n",
      "M0 mechanism shape: (2, 12)\n",
      "Alpha_s shape: (4, 12)\n",
      "Alpha_t shape: (2, 2)\n",
      "All JS distances: [0.2920517223899879, 0.14829984047274664, 0.19372726395553316, 0.2920517223899879, 0.2920517223899879, 0.14829984047274675, 0.19372726395553327, 0.2920517223899879, 0.2920517223899879, 0.14829984047274675, 0.19372726395553327, 0.2920517223899879]\n",
      "\n",
      "Abstraction error: 0.2920517223899879\n",
      "\n",
      "M1: ['X', 'Z'] -> ['Y']\n",
      "M0: ['A', 'D'] -> ['B', 'C']\n",
      "M1 mechanism shape: (2, 4)\n",
      "M0 mechanism shape: (4, 6)\n",
      "Alpha_s shape: (4, 6)\n",
      "Alpha_t shape: (2, 4)\n",
      "All JS distances: [0.3679245494483625, 0.36792454944836245, 0.5723641752371845, 0.5723641752371845, 0.523792390695269, 0.523792390695269]\n",
      "\n",
      "Abstraction error: 0.5723641752371845\n",
      "\n",
      "\n",
      "OVERALL ABSTRACTION ERROR: 0.5841050035045615\n"
     ]
    }
   ],
   "source": [
    "err = A.evaluate_abstraction_error(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the abstraction error for the intervention of interest is $E_\\alpha(Z,X) \\approx 0.15$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "We generate data from the low-level model for learning. This will provide the first part $(\\mathbf{a},\\mathbf{d})$ of our datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A  B  C  D\n",
      "0     0  0  0  0\n",
      "1     0  0  0  0\n",
      "2     2  0  1  0\n",
      "3     0  0  1  1\n",
      "4     0  1  0  1\n",
      "...  .. .. .. ..\n",
      "9995  0  0  0  0\n",
      "9996  0  0  0  0\n",
      "9997  0  1  1  1\n",
      "9998  0  0  0  0\n",
      "9999  0  1  0  1\n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "M0do = M0.do('A')\n",
    "lowlevel_data = M0do.simulate(n_samples=n_samples, show_progress=False)\n",
    "print(lowlevel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we generate data from the high-level model. This is meant to provide the second part $(\\mathbf{x},\\mathbf{z})$ of our datapoints. Notice that we need to align correctly our datapoints since we want $\\mathbf{x} = \\alpha_{X}(\\mathbf{a})$; therefore we generate the datapoints one by one.\n",
    "\n",
    "**TODO: this generation method is highly inefficient!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_X = np.array([[1,0,0],[0,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Z  X  Y\n",
      "0   0  0  0\n",
      "0   1  0  0\n",
      "0   1  1  1\n",
      "0   1  0  0\n",
      "0   1  0  0\n",
      ".. .. .. ..\n",
      "0   1  0  0\n",
      "0   0  0  0\n",
      "0   0  0  0\n",
      "0   0  0  0\n",
      "0   0  0  0\n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "M1do = M1.do('X')\n",
    "\n",
    "high_level_samples = [M1do.simulate(n_samples=1, evidence={'X': np.where(alphas['X'][:,lowlevel_data.loc[i]['A']]==1)[0][0]}, show_progress=False) for i in range(lowlevel_data.shape[0])]\n",
    "highlevel_data = pd.concat(high_level_samples)\n",
    "    \n",
    "print(highlevel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning model definition\n",
    "\n",
    "We define a neural network with the modules and loss functions described above.\n",
    "\n",
    "We start with some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "hiddensize = 4\n",
    "num_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1,hiddensize)\n",
    "        self.sm1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hiddensize,1)\n",
    "        self.lowmech = nn.Sigmoid()\n",
    "        \n",
    "        self.fc3 = nn.Linear(1,hiddensize)\n",
    "        self.sm2 = nn.Sigmoid()\n",
    "        self.fc4 = nn.Linear(hiddensize,1)\n",
    "        self.absS = nn.Sigmoid()\n",
    "        \n",
    "        self.fc5 = nn.Linear(1,hiddensize)\n",
    "        self.sm3 = nn.Sigmoid()\n",
    "        self.fc6 = nn.Linear(hiddensize,1)\n",
    "        self.highmech = nn.Sigmoid()\n",
    "        \n",
    "        self.fc7 = nn.Linear(1,hiddensize)\n",
    "        self.sm4 = nn.Sigmoid()\n",
    "        self.fc8 = nn.Linear(hiddensize,1)\n",
    "        self.absC = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_lowS, input_highS):\n",
    "        out = self.fc1(input_lowS)\n",
    "        out = self.sm1(out)\n",
    "        out = self.fc2(out)\n",
    "        mech_lowS = self.lowmech(out)\n",
    "        \n",
    "        out = self.fc3(input_lowS)\n",
    "        out = self.sm2(out)\n",
    "        out = self.fc4(out)\n",
    "        abs_lowS = self.absS(out)\n",
    "        \n",
    "        out = self.fc5(input_highS)\n",
    "        out = self.sm3(out)\n",
    "        out = self.fc6(out)\n",
    "        mech_highS = self.highmech(out)\n",
    "        \n",
    "        out = self.fc5(abs_lowS)\n",
    "        out = self.sm3(out)\n",
    "        out = self.fc6(out)\n",
    "        mech_abs_lowS = self.highmech(out)\n",
    "        \n",
    "        out = self.fc7(mech_lowS)\n",
    "        out = self.sm4(out)\n",
    "        out = self.fc8(out)\n",
    "        abs_mech_lowS = self.absS(out)\n",
    "        \n",
    "        return mech_lowS, abs_lowS, mech_highS, abs_mech_lowS, mech_abs_lowS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model and its optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss_lowmech = nn.MSELoss()\n",
    "loss_highmech = nn.MSELoss()\n",
    "loss_abstraction = nn.MSELoss()\n",
    "loss_commutativity = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/5000], Loss: 0.4967\n",
      "Epoch [1000/5000], Loss: 0.4868\n",
      "Epoch [1500/5000], Loss: 0.4855\n",
      "Epoch [2000/5000], Loss: 0.4851\n",
      "Epoch [2500/5000], Loss: 0.4849\n",
      "Epoch [3000/5000], Loss: 0.4848\n",
      "Epoch [3500/5000], Loss: 0.4848\n",
      "Epoch [4000/5000], Loss: 0.4848\n",
      "Epoch [4500/5000], Loss: 0.4847\n",
      "Epoch [5000/5000], Loss: 0.4847\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    input_lowS = torch.from_numpy(np.expand_dims(lowlevel_data['A'].to_numpy(dtype=np.float32),axis=1))\n",
    "    input_highS = torch.from_numpy(np.expand_dims(highlevel_data['X'].to_numpy(dtype=np.float32),axis=1))\n",
    "    target_lowC = torch.from_numpy(np.expand_dims(lowlevel_data['D'].to_numpy(dtype=np.float32),axis=1))\n",
    "    target_highC = torch.from_numpy(np.expand_dims(highlevel_data['Z'].to_numpy(dtype=np.float32),axis=1))\n",
    "    \n",
    "    mech_lowS, abs_lowS, mech_highS, abs_mech_lowS, mech_abs_lowS  = model(input_lowS, input_highS)\n",
    "    \n",
    "    l1 = loss_lowmech(mech_lowS,target_lowC)\n",
    "    l2 = loss_highmech(mech_highS,target_highC)\n",
    "    l3 = loss_abstraction(abs_lowS,input_highS)\n",
    "    l4 = loss_commutativity(abs_mech_lowS,mech_abs_lowS)                                \n",
    "    loss = l1+l2+l3\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 500 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We define a new function to print out the result of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smalltest(model, test_lowA,test_highX):\n",
    "    mech_lowA, abs_lowA, mech_highX, abs_mech_lowA, mech_abs_lowA = model(test_lowA,test_highX)\n",
    "    \n",
    "    print('-------------------------')\n",
    "    print('Given I perform the intervention A={0}'.format(test_lowA.item()))\n",
    "    print('The prediction of the low level mechanism P_M0 (D=1 | A={0}) outputs {1}'.format(test_lowA.item(),mech_lowA.item()))\n",
    "    \n",
    "    print('\\nThe prediction of the abstraction to high-level alpha_X({0}) outputs {1} against the expected {2}'.format(test_lowA.item(),abs_lowA.item(),test_highX.item()))\n",
    "    \n",
    "    print('\\nThe prediction of the high level mechanism P_M1 (Z=1 | X={0}) outputs {1} using the input {2}'.format(test_highX.item(),mech_highX.item(),test_highX.item()))\n",
    "    print('The prediction of the high level mechanism P_M1 (Z=1 | X={0}) outputs {1} using the transformed input {2}'.format(abs_lowA.item(),mech_abs_lowA.item(),abs_lowA.item()))\n",
    "    \n",
    "    print('\\nThe upper path produces {0}'.format(mech_abs_lowA.item()))\n",
    "    print('The lower path produces {0}'.format(abs_mech_lowA.item()))\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check what our network produces with an input of $(\\mathbf{a}=0,\\mathbf{x}=0)$. As a reference we output the value of the mechanism $\\mathcal{M}[\\phi_{{\\tilde{D}}}] = P(D|do(A))$ and $\\mathcal{M'}[\\phi_{Z}] = P(Z|do(X))$ which we are trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5825 , 0.4175 ],\n",
       "       [0.6    , 0.4    ],\n",
       "       [0.55125, 0.44875]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferM0 = VariableElimination(M0)\n",
    "M0_joint_AD = inferM0.query(['A','D'],show_progress=False)\n",
    "M0_joint_A = inferM0.query(['A'],show_progress=False)\n",
    "M0_cond_DA = M0_joint_AD / M0_joint_A\n",
    "M0_cond_DA.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4025, 0.5975],\n",
       "       [0.39  , 0.61  ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferM1 = VariableElimination(M1)\n",
    "M1_joint_XZ = inferM1.query(['X','Z'],show_progress=False)\n",
    "M1_joint_X = inferM1.query(['X'],show_progress=False)\n",
    "M1_cond_ZX = M1_joint_XZ / M1_joint_X\n",
    "M1_cond_ZX.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Given I perform the intervention A=0.0\n",
      "The prediction of the low level mechanism P_M0 (D=1 | A=0.0) outputs 0.41887593269348145\n",
      "\n",
      "The prediction of the abstraction to high-level alpha_X(0.0) outputs 0.005152891390025616 against the expected 0.0\n",
      "\n",
      "The prediction of the high level mechanism P_M1 (Z=1 | X=0.0) outputs 0.5912103652954102 using the input 0.0\n",
      "The prediction of the high level mechanism P_M1 (Z=1 | X=0.005152891390025616) outputs 0.5912752151489258 using the transformed input 0.005152891390025616\n",
      "\n",
      "The upper path produces 0.5912752151489258\n",
      "The lower path produces 0.630756676197052\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "test_lowA = torch.from_numpy(np.array([0.],dtype=np.float32))\n",
    "test_highX = torch.from_numpy(np.array([0.],dtype=np.float32))\n",
    "\n",
    "smalltest(model, test_lowA,test_highX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network roughly learnt our diagram:\n",
    "- The prediction $\\hat{P}_\\mathcal{M}(D=1 \\vert do(A=0))$ is roughly around $0.4175$;\n",
    "- The prediction $\\alpha_X(0)$ is close to $0$;\n",
    "- The prediction $\\hat{P}_\\mathcal{M'}(Z=1 \\vert do(S=1))$ is roughly around $0.5975$ when using the input data $\\mathbf{x}$; it is further away when using $\\alpha_X(\\mathbf{a})$ because of accumulation of approximations.\n",
    "- The results along the upper and lower path are sort of close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Given I perform the intervention A=1.0\n",
      "The prediction of the low level mechanism P_M0 (D=1 | A=1.0) outputs 0.4125082492828369\n",
      "\n",
      "The prediction of the abstraction to high-level alpha_X(1.0) outputs 0.9892100691795349 against the expected 1.0\n",
      "\n",
      "The prediction of the high level mechanism P_M1 (Z=1 | X=1.0) outputs 0.6058823466300964 using the input 1.0\n",
      "The prediction of the high level mechanism P_M1 (Z=1 | X=0.9892100691795349) outputs 0.6057053208351135 using the transformed input 0.9892100691795349\n",
      "\n",
      "The upper path produces 0.6057053208351135\n",
      "The lower path produces 0.6308521032333374\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "test_lowA = torch.from_numpy(np.array([1.],dtype=np.float32))\n",
    "test_highX = torch.from_numpy(np.array([1.],dtype=np.float32))\n",
    "\n",
    "smalltest(model, test_lowA,test_highX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Given I perform the intervention A=2.0\n",
      "The prediction of the low level mechanism P_M0 (D=1 | A=2.0) outputs 0.4495827555656433\n",
      "\n",
      "The prediction of the abstraction to high-level alpha_X(2.0) outputs 0.9981436729431152 against the expected 1.0\n",
      "\n",
      "The prediction of the high level mechanism P_M1 (Z=1 | X=1.0) outputs 0.6058823466300964 using the input 1.0\n",
      "The prediction of the high level mechanism P_M1 (Z=1 | X=0.9981436729431152) outputs 0.6058518886566162 using the transformed input 0.9981436729431152\n",
      "\n",
      "The upper path produces 0.6058518886566162\n",
      "The lower path produces 0.6302937269210815\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "test_lowA = torch.from_numpy(np.array([2.],dtype=np.float32))\n",
    "test_highX = torch.from_numpy(np.array([1.],dtype=np.float32))\n",
    "\n",
    "smalltest(model, test_lowA,test_highX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: (i) review architecture and loss of NN; (ii) consider other problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.038px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
